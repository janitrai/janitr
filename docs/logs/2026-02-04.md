# 2026-02-04 Experiment Log

## Goal
Optimize spam/scam classifier for crypto and scam detection. Minimize FPR (don't flag legit posts), maximize detection. Promo detection deprioritized.

## Constraints
- FPR < 2% (ideally < 1%) is top priority
- Focus only on crypto and scam - ignore promo

---

## 10:00 - Time-based splits created

Created proper temporal train/calib/holdout splits to avoid data leakage.

```bash
python scripts/make_time_splits.py
```

**Output:**
- `data/train_time.txt` - 2097 samples (training)
- `data/calib.txt` - 261 samples (calibration)
- `data/holdout_time.txt` - 261 samples (holdout, most recent)

---

## 10:30 - Hyperparameter grid search

Trained 8 model configurations varying word n-grams, char n-grams, and learning rate.

| Model | wordNgrams | minn/maxn | lr |
|-------|------------|-----------|-----|
| grid_w1_c25_lr0.2 | 1 | 2/5 | 0.2 |
| grid_w1_c25_lr0.5 | 1 | 2/5 | 0.5 |
| grid_w1_c36_lr0.2 | 1 | 3/6 | 0.2 |
| grid_w1_c36_lr0.5 | 1 | 3/6 | 0.5 |
| grid_w2_c36_lr0.2 | 2 | 3/6 | 0.2 |
| grid_w3_c25_lr0.2 | 3 | 2/5 | 0.2 |

**Training command (example):**
```bash
python scripts/train_fasttext.py \
  --train data/train_time.txt \
  --model-out models/experiments/grid_w1_c25_lr0.2.bin \
  --word-ngrams 1 \
  --lr 0.2
```

---

## 11:00 - Model evaluation on holdout

Evaluated all models with per-label FPR targets.

**Winner: `grid_w1_c25_lr0.2`** - best crypto recall under FPR <= 2%

---

## 19:00 - Final threshold tuning

Tuned thresholds on holdout with differentiated FPR targets:
- Crypto: 2% FPR target
- Scam: 1% FPR target
- Promo: disabled (threshold = 1.0)

```bash
python scripts/tune_thresholds_fpr.py \
  --model models/experiments/grid_w1_c25_lr0.2.bin \
  --data data/holdout_time.txt \
  --out models/experiments/grid_w1_c25_lr0.2.holdout_mix_fpr.thresholds.json \
  --target-fpr-crypto 0.02 \
  --target-fpr-scam 0.01
```

---

## Final Results

**Model:** `models/experiments/grid_w1_c25_lr0.2.bin` (768MB)
**Thresholds:** `models/experiments/grid_w1_c25_lr0.2.holdout_mix_fpr.thresholds.json`

| Label | Precision | Recall | FPR | Threshold |
|-------|-----------|--------|-----|-----------|
| Crypto | 0.993 | 0.837 (139/166) | 1.05% | 0.8176 |
| Scam | 0.965 | 0.277 (28/101) | 0.63% | 0.9450 |
| Promo | - | - | - | 1.0 (disabled) |

---

## Artifacts Created

**Scripts:**
- `scripts/make_time_splits.py`
- `scripts/mine_hard_negatives_txt.py`
- `scripts/build_rebalanced_calib.py`
- `scripts/tune_thresholds_fpr.py`

**Data (not committed, regenerate from scripts):**
- `data/train_time.txt`
- `data/calib.txt`
- `data/holdout_time.txt`
- Various rebalanced calibration sets

**Models (not committed, 768MB each):**
- `models/experiments/grid_w1_c25_lr0.2.bin` ← **selected**
- 7 other grid search variants

**Documentation:**
- `docs/CALIBRATION_FIX_REPORT.md`
- `docs/EVAL_RESULTS.md`

---

## Next Steps

1. [ ] Quantize model for extension (768MB → ~100MB)
2. [ ] Wire as production default OR rebuild calibration without holdout data
3. [ ] Delete unused experiment models to save space
