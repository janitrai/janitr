---
title: Daily Log 2026-02-14
author: bob <unknown@example.com>
date: 2026-02-14
tz: Europe/Berlin
created: 2026-02-14T00:36:03+01:00
last_section: 2026-02-14T23:45:30+01:00
updated: 2026-02-14T23:45:30+01:00
---

## 00:36

Starting DAPT-consumption A/B step (B arm) with tight controls: run teacher initialized from models/teacher_dapt using seed 13, then calibrate/cache/distill/export/quantize/evaluate in isolated experiment paths. Baseline reference remains models/student_holdout_eval.json (scam p=0.9512 r=0.7800 fpr=0.0122).

DAPT A/B one-seed attempt blocked by train_transformer_teacher.py guard (datasets <=5k rows require 3 seeds). Adjusted plan to run DAPT arm with seeds 13,42,7 for compliance and comparability against current 3-seed baseline.

## 00:48

Completed full DAPT A/B experiment (3-seed teacher + full student chain) in isolated run directory:

- models/student_experiments/2026-02-14-dapt-ab-seeds-13-42-7

Pipeline executed end-to-end on GPU:

1. train_transformer_teacher.py --teacher-init-path models/teacher_dapt --seeds 13,42,7
2. calibrate_teacher.py
3. cache_teacher_logits.py
4. train_transformer_student_distill.py
5. export_transformer_student_onnx.py
6. quantize_transformer_student.py
7. evaluate_transformer.py

Key holdout comparison vs baseline models/student_holdout_eval.json:

- Scam precision: 0.9512 -> 0.9024 (worse)
- Scam recall: 0.7800 -> 0.7400 (worse)
- Scam FPR: 0.0122 -> 0.0244 (worse; doubled)
- Scam F1: 0.8571 -> 0.8132 (worse)
- Topic F1: 0.8000 -> 0.8430 (better)
- Macro F1: 0.8504 -> 0.8514 (slightly better)
- Exact match: 0.8598 -> 0.8645 (slightly better)

Thresholds shifted materially (scam 0.61 -> 0.78, topic 0.60 -> 0.50), indicating non-trivial calibration behavior change.

Decision: reject DAPT as default teacher init for now because primary objective (scam recall at low FPR) regressed materially.

## 00:54

Starting non-DAPT student distillation sweep with fixed teacher logits (models/teacher_logits_train.npz + models/teacher_logits_valid.npz), fixed data splits, and fixed preprocessing.
Grid:

- distill_temp: 2.0, 3.0, 4.0
- alpha (hard-loss weight): 0.5, 0.7, 0.85
  Selection criterion: maximize scam recall while keeping holdout scam FPR <= 0.02.

Completed non-DAPT distillation hyperparameter sweep (9 runs) using fixed teacher logits and fixed data splits.
Experiment root:

- models/student_experiments/2026-02-14-student-sweep-nondapt

Grid:

- distill_temp: 2.0, 3.0, 4.0
- alpha (hard loss weight): 0.5, 0.7, 0.85
- seed fixed at 42

For each run, executed:

1. train_transformer_student_distill.py
2. evaluate_transformer.py (threshold tuning on valid, holdout reporting)

Baseline reference (models/student_holdout_eval.json):

- scam precision=0.9512
- scam recall=0.7800
- scam fpr=0.0122
- scam f1=0.8571
- macro f1=0.8504

Outcome summary:

- No sweep config improved scam recall at or below 2% FPR vs baseline.
- Best under FPR<=0.02 was t4_a05 (temp=4, alpha=0.5):
  - scam precision=0.9231
  - scam recall=0.7200
  - scam fpr=0.0183
  - scam f1=0.8090
- Highest recall overall was t3_a05 (0.7400) but FPR rose to 0.0305.

Decision:

- Keep current baseline student as default; this sweep regressed the primary objective (scam recall/F1 at low FPR).
- Next likely high-ROI training-only step is hard-example reweighting / focal-style loss on existing labels, since pure temp/alpha distillation sweeps did not improve the scam operating point.

## 01:07

Starting long-duration baseline student training experiment:

- Same baseline setup (non-DAPT teacher logits, baseline architecture/hyperparams)
- Increased epochs from 10 to 60
- Will capture epoch-by-epoch valid_macro_f1 trend and compare final holdout metrics against baseline.

Completed long-duration baseline training experiment:

- models/student_experiments/2026-02-14-baseline-longtrain-epochs60
- Baseline student config, 60 epochs (seed 42), non-DAPT teacher logits
- Full eval run completed

Epoch trend (valid_macro_f1 highlights):

- epoch 1: 0.6429
- epoch 2: 0.8279
- epoch 3: 0.8357
- epoch 10: 0.8270
- epoch 20: 0.8268
- epoch 30: 0.8341
- epoch 40: 0.8310
- epoch 50: 0.8241
- best epoch: 51 with valid_macro_f1=0.8430
- epoch 60: 0.8252

Holdout comparison vs baseline models/student_holdout_eval.json:

- scam precision: 0.9512 -> 0.8947 (worse)
- scam recall: 0.7800 -> 0.6800 (worse)
- scam fpr: 0.0122 -> 0.0244 (worse; doubled)
- scam f1: 0.8571 -> 0.7727 (worse)
- topic f1: 0.8000 -> 0.8293 (better)
- macro f1: 0.8504 -> 0.8351 (worse)
- exact match: 0.8598 -> 0.8551 (worse)

Thresholds shifted upward materially (scam 0.61 -> 0.83; topic 0.60 -> 0.72).

Conclusion: training much longer did not help the primary operating point and appears to hurt scam generalization / calibration despite occasional late validation peaks.

## 10:40

Starting student capacity sweep to measure size/performance tradeoff.
Controls fixed: same teacher logits, same data splits, same training recipe (distill_temp=3.0, alpha=0.5, epochs=10, seed=42).
Variable changed: student architecture only.
For each architecture: train -> evaluate -> export ONNX -> quantize int8, then compare holdout scam metrics and artifact sizes.

Capacity sweep interruption: configuration h256_l6_a4_i1024 failed during student training.
Error: IndexError in train_transformer_student_distill.py when accessing teacher hidden states (index 4 out of bounds for size 4).
Cause: current distillation implementation expects student layer count <= teacher hidden layer cache depth (4).
Action: continue sweep with valid 4-layer architectures only to isolate size effects.

Completed student capacity sweep experiment.
Experiment root: models/student_experiments/2026-02-14-capacity-sweep

Valid architectures run (same training recipe, same teacher logits):

- h128_l4_a4_i512
- h192_l4_a4_i768
- h256_l4_a4_i1024
- h384_l4_a6_i1536

For each config executed: train -> evaluate -> ONNX export -> int8 quantize.

Holdout outcome summary vs baseline models/student_holdout_eval.json:

- h128_l4_a4_i512: int8 1.93MB, scam recall 0.74, scam FPR 0.0366, scam F1 0.7957
- h192_l4_a4_i768: int8 3.39MB, scam recall 0.68, scam FPR 0.0183, scam F1 0.7816
- h256_l4_a4_i1024: int8 5.23MB, scam recall 0.70, scam FPR 0.0305, scam F1 0.7778
- h384_l4_a6_i1536: int8 10.02MB, scam recall 0.72, scam FPR 0.0122, scam F1 0.8182

Baseline reference remains stronger for primary objective:

- baseline scam recall 0.78 at scam FPR 0.0122, scam F1 0.8571

Observed trend in this sweep: larger models increased size substantially and improved some metrics vs smaller sweep configs, but did not beat the established baseline operating point.

Failure encountered and diagnosed:

- h256_l6_a4_i1024 failed with IndexError due teacher hidden-state cache depth (4) being smaller than student layer count (6); current distillation implementation requires student layers <= cached teacher layer depth.

## 11:29

Backing up current split files before holdout expansion:

- data/split_backups/2026-02-14-holdout-expand/{train,valid,holdout}.{jsonl,txt}
  Planned new split ratios from data/sample.jsonl:
- train=0.60, valid=0.10, calib=0.05, holdout=0.25
  Goal: holdout around 1k rows with roughly 200-300 scam examples.

Completed holdout expansion.

Actions taken:

1. Rebuilt stratified splits from data/sample.jsonl using ratios:
   - train=0.60, valid=0.10, calib=0.05, holdout=0.25
     Command: uv run --project scripts python scripts/make_stratified_splits.py --input data/sample.jsonl --train-ratio 0.60 --valid-ratio 0.10 --calib-ratio 0.05 --holdout-ratio 0.25 --seed 42
2. Regenerated transformer prepared splits:
   - uv run --project scripts python scripts/prepare_transformer_data.py

Resulting prepared counts:

- train: 2566 (scam 587, clean 1216, topic_crypto 763)
- valid: 428 (scam 98, clean 203, topic_crypto 127)
- holdout: 1069 (scam 245, clean 506, topic_crypto 318)

Holdout target achieved: >1k rows with scam count in the 200-300 range.

Note: Existing model/eval artifacts were produced on the old holdout and are no longer directly comparable to new holdout metrics; retraining/re-evaluation is needed for apples-to-apples numbers.

## 12:50

Starting full retrain chain on expanded holdout splits.
Run root: models/student_experiments/2026-02-14-retrain-expanded-holdout
Steps: teacher(3 seeds 13,42,7) -> calibrate -> cache logits -> student distill -> ONNX export -> int8 quantize -> evaluate.
Using new prepared splits:

- data/transformer/train.prepared.jsonl (2566)
- data/transformer/valid.prepared.jsonl (428)
- data/transformer/holdout.prepared.jsonl (1069)

## 13:01

Added expanded-holdout retrain evaluation report:

- docs/reports/2026-02-14-tiny-transformer-expanded-holdout-retrain-report.md
  Included: updated split sizes, full command chain, required teacher config (cardiffnlp/twitter-roberta-large-2022-154m, seeds 13/42/7), teacher and student holdout metrics, artifact inventory/sizes, and comparability caveat versus prior small-holdout baseline.
  Preparing docs-only commit; model/data binaries remain untracked and will not be committed.

## 13:10

Ran apples-to-apples comparison on expanded holdout (1069 rows) by evaluating prior baseline student checkpoint (models/student) with current splits.
Command: uv run --project scripts python scripts/evaluate_transformer.py --student-dir models/student --train data/transformer/train.prepared.jsonl --valid data/transformer/valid.prepared.jsonl --holdout data/transformer/holdout.prepared.jsonl --thresholds-out models/student_experiments/2026-02-14-baseline-on-expanded-holdout/thresholds.transformer.json --out models/student_experiments/2026-02-14-baseline-on-expanded-holdout/student_holdout_eval.json
Result: prior baseline is substantially better than retrain-expanded-holdout run on the same holdout.

- Baseline-on-expanded: scam P/R/F1/FPR = 0.9488 / 0.8327 / 0.8870 / 0.0133; topic F1 = 0.8916; macro F1 = 0.9068; exact = 0.9149
- Retrain-expanded-holdout run: scam P/R/F1/FPR = 0.9383 / 0.6204 / 0.7469 / 0.0121; topic F1 = 0.7695; macro F1 = 0.7967; exact = 0.8176
  Primary regression is recall/F1 collapse at similar scam FPR.

## 13:27

Root-cause analysis for apparent regression after holdout expansion:

- Prior baseline checkpoint (models/student) was trained on old split before reshuffle.
- New expanded holdout (1069) overlaps heavily with old splits:
  - new holdout ∩ old train = 847
  - new holdout ∩ old valid = 72
  - new holdout ∩ old holdout = 10
  - total overlap with old(any split) = 929 / 1069 (86.9%)
- Therefore baseline-vs-retrain comparison on expanded holdout is contaminated; baseline appears artificially strong because most of that holdout was previously seen by baseline during prior training/validation cycle.
- New split itself is clean (new holdout ∩ new train = 0; new holdout ∩ new valid = 0).
- Retrain also had reduced supervision due split change: old train 3421 -> new train 2566 (-25%), scam samples 783 -> 587 (-25%).
  Interpretation: large measured regression versus prior baseline is largely an illusion from leakage; residual degradation likely from reduced train set and untuned student distillation settings for the new split regime.

## 15:00

Starting clean benchmark rebuild workflow on user request:

1. freeze current split as benchmark reference
2. add overlap/leakage guard
3. retrain fastText baseline from scratch on frozen split
4. retrain transformer teacher->student baseline from scratch on same frozen split
5. produce apples-to-apples comparison report.

Frozen benchmark split created at dataset/benchmarks/2026-02-14_expanded_holdout:

- train/valid/calib/holdout ID lists
- MANIFEST.json with row counts, label support, and SHA256 hashes
  Added scripts/check_split_leakage.py and validated:
- current split pairwise overlap is zero across train/valid/calib/holdout
- previous-vs-current contamination check reproduces overlap in new holdout (847 old-train, 72 old-valid, 10 old-holdout)

Trained fastText baseline from scratch on frozen split data/train.txt.

- Model: models/benchmarks/2026-02-14_expanded_holdout/fasttext/scam_detector.bin
- Calibrated thresholds on data/calib.txt with target FPR 0.02 for topic_crypto/scam
- Evaluated on data/holdout.txt and wrote structured eval JSON:
  models/benchmarks/2026-02-14_expanded_holdout/fasttext/holdout_eval.json

## 15:12

Completed full transformer benchmark retrain on frozen split (teacher->calibration->logits->student->onnx->int8->eval).
Run root: models/benchmarks/2026-02-14_expanded_holdout/transformer
Teacher: cardiffnlp/twitter-roberta-large-2022-154m, seeds 13/42/7
Student holdout (torch): scam precision/recall/fpr/f1 = 0.9371 / 0.6082 / 0.0121 / 0.7376; macro_f1=0.7998; exact=0.8232
Student holdout (int8 onnx): scam precision/recall/fpr/f1 = 0.9375 / 0.6122 / 0.0121 / 0.7407; macro_f1=0.8013; exact=0.8241

Wrote frozen-split benchmark report:

- docs/reports/2026-02-14-frozen-split-fasttext-vs-transformer-benchmark.md
  Includes: split freeze artifacts, leakage checks, full retrain commands, side-by-side holdout metrics (fastText vs transformer torch/int8), and benchmark conclusion.

Extended benchmark with fastText production-profile quantization:

- reduced fastText model to scam_detector.ftz (cutoff=1000,dsub=8), size ~123KB
- evaluated quantized fastText on holdout and added structured artifact holdout_eval_ftz.json
- updated report to compare transformer-int8 against fastText-ftz (production-relevant) in addition to noting unquantized .bin size.

## 16:51

Updated the general visualization skill instructions in `.agent/skills/general-visualization/SKILL.md` to enforce comparison formatting defaults:

- alternatives on horizontal axis (columns) and metrics on vertical axis (rows)
- 2 significant figures by default
- omit leading zero for decimals under 1 (e.g., `.84`, `-.07`)
- always include explicit better/worse statements per metric and overall

This change was requested to standardize side-by-side experiment reporting output.

## 17:12

Updated `.agent/skills/general-visualization/SKILL.md` notation rules for pairwise metric comparisons:

- removed required better/worse verdict column
- better value must be bolded
- added impact marker scale based on relative improvement:
  - 0-<1%: = (roughly equal)
  - 1-<5%: \*
  - 5-<10%: \*\*
  - 10-50%: \*\*\*
  - > 50%: \*\*\*\*
- retained orientation and formatting defaults (alternatives as columns, metrics as rows, 2 sig figs, omit leading zero for decimals under 1)

Also updated `.agent/skills/general-visualization/agents/openai.yaml` prompt text to match the new notation.

Follow-up fix in `.agent/skills/general-visualization/SKILL.md`: corrected example rows in the delta template so the better value is bolded consistently (including when baseline is better and for lower-is-better metrics like latency).

## 17:26

Adjusted general visualization skill for terminal-first rendering:

- no HTML tags
- decimal-point alignment requirement added
- impact marker now attaches directly to the better value (suffix)
- tie handling uses `=` suffix on both values
- removed separate impact column requirement from templates

Also updated agents metadata prompt text to match the new inline-star notation.

## 17:33

Updated general visualization skill notation per user preference:

- removed `=` tie marker entirely
- ties/near-ties (<1%) now have no marker
- winner markers remain `*`, `**`, `***`, `****` on better value only
- `improvement` column now uses regular integer percentages (e.g., `0%`, `2%`, `14%`) instead of decimal percentages

Updated both SKILL.md and agents/openai.yaml prompt metadata.

## 17:42

Updated general visualization skill for comparison formatting:

- changed comparison delta to signed absolute change (`right - left`) in percentage points
- `change` column now displays signed integer percentages with aligned `%`
- retained stars on the better value but required fixed-width star slot so star columns align
- kept alternatives horizontal and metrics vertical, with decimal-point alignment and 2 sig figs

Updated both SKILL.md and agents/openai.yaml to reflect these rules.

Minor follow-up in visualization skill: clarified that `change` is shown as signed integer percentages (rounded) with aligned percent signs.

## 23:30

Implemented four transformer pipeline refactors across scripts:

1. Shared student runtime module

- Added `scripts/student_runtime.py` with shared `TinyStudentModel`, `build_student_bert_config`, and `load_student_from_dir`.
- Removed duplicated student model/config loading logic from:
  - `scripts/train_transformer_student_distill.py`
  - `scripts/export_transformer_student_onnx.py`
  - `scripts/evaluate_transformer.py`

2. Artifact identity manifests + fail-fast compatibility checks

- Added artifact identity/hash helpers to `scripts/transformer_common.py`:
  - `stable_object_hash`, `current_git_commit`, `utc_now_iso`, `hash_label_map`, `hash_prepared_rows`, `parse_seed_csv`.
- Teacher training now emits `teacher_id` + `teacher_manifest.json`:
  - `scripts/train_transformer_teacher.py`
- Calibration now requires teacher manifest and writes `calibration_id` + metadata in calibration payload:
  - `scripts/calibrate_teacher.py`
- Logits cache now requires teacher/calibration IDs to match, enforces exact seed-set match, writes sidecar cache metadata (`*.npz.meta.json`) with `logits_cache_id`, and fail-fasts on existing metadata mismatches:
  - `scripts/cache_teacher_logits.py`
- Student distillation now requires cache metadata, verifies teacher/calibration consistency, seed consistency, label-map hash consistency, and split-hash consistency before training:
  - `scripts/train_transformer_student_distill.py`
- Student config now stores source artifact lineage (`teacher_id`, `calibration_id`, cache IDs, seeds, split hashes).

3. Removed global side effect from student training

- Removed write to global `models/student_eval.json`.
- Student eval is now only written inside the run output directory (`output_dir/student_eval.json`).

4. Centralized metric + threshold logic

- Added shared metrics/threshold helpers in `scripts/transformer_common.py`:
  - `format_one_vs_all_metrics`
  - `summarize_label_predictions`
  - `predict_labels_from_probs`
  - `tune_thresholds_for_scam_fpr`
- Updated scripts to use shared helpers:
  - `scripts/evaluate_transformer.py`
  - `scripts/train_transformer_student_distill.py`
  - `scripts/train_transformer_teacher.py`

Additional evaluation-stage guard

- `scripts/evaluate_transformer.py` now validates `student_config.json` provenance (`source_artifacts`) by default and supports `--allow-missing-provenance` for legacy artifacts.

Validation performed

- Compiled modified scripts with:
  `uv run --project scripts python -m py_compile scripts/transformer_common.py scripts/student_runtime.py scripts/train_transformer_student_distill.py scripts/evaluate_transformer.py scripts/export_transformer_student_onnx.py scripts/train_transformer_teacher.py scripts/calibrate_teacher.py scripts/cache_teacher_logits.py`
- Ran CLI help smoke checks for updated scripts:
  - `train_transformer_student_distill.py --help`
  - `evaluate_transformer.py --help`
  - `cache_teacher_logits.py --help`
  - plus earlier help checks for teacher/calibrate/export.

Validation/fix follow-up:

- Running `export_transformer_student_onnx.py` after runtime unification initially failed with:
  `TypeError: TinyStudentModel.forward() takes 3 positional arguments but 5 were given`.
- Root cause: ONNX tracer invoked `forward` with optional flags positionally; shared model used keyword-only args.
- Fix: updated `scripts/student_runtime.py` forward signature to accept positional optional args (`output_hidden_states`, `return_dict`).
- Re-ran export parity check successfully:
  - ONNX export completed
  - parity mean delta: 0.000000
  - label agreement: 100.0% (1000/1000)

## 23:37

Reran frozen-split benchmark evaluations after refactor updates.

Transformer (models/benchmarks/2026-02-14_expanded_holdout/transformer):

- torch eval rerun completed
  - thresholds: scam=0.87, topic_crypto=0.50
  - holdout scam precision=0.9371 recall=0.6082 fpr=0.0121
  - output: student_holdout_eval.json
- int8 ONNX eval rerun completed
  - thresholds: scam=0.87, topic_crypto=0.50
  - holdout scam precision=0.9375 recall=0.6122 fpr=0.0121
  - output: student_holdout_eval_int8.json

FastText reference on same holdout was rerun for sanity and remains consistent with prior benchmark log:

- scam precision=0.913 recall=0.555 fpr=0.016 (per-label thresholds, no allow-empty)

## 23:45

Updated general visualization skill header policy:

- comparison table headers must use actual alternative names directly
- removed generic Left/Right header convention
- removed need for separate left/right mapping note
- clarified `change` is computed as second alternative column minus first alternative column

Also updated skill metadata prompt (`agents/openai.yaml`) to enforce named headers and second-minus-first change semantics.
