---
title: Daily Log 2026-02-14
author: bob <unknown@example.com>
date: 2026-02-14
tz: Europe/Berlin
created: 2026-02-14T00:36:03+01:00
last_section: 2026-02-14T15:12:42+01:00
updated: 2026-02-14T15:14:30+01:00
---

## 00:36

Starting DAPT-consumption A/B step (B arm) with tight controls: run teacher initialized from models/teacher_dapt using seed 13, then calibrate/cache/distill/export/quantize/evaluate in isolated experiment paths. Baseline reference remains models/student_holdout_eval.json (scam p=0.9512 r=0.7800 fpr=0.0122).

DAPT A/B one-seed attempt blocked by train_transformer_teacher.py guard (datasets <=5k rows require 3 seeds). Adjusted plan to run DAPT arm with seeds 13,42,7 for compliance and comparability against current 3-seed baseline.

## 00:48

Completed full DAPT A/B experiment (3-seed teacher + full student chain) in isolated run directory:
- models/student_experiments/2026-02-14-dapt-ab-seeds-13-42-7

Pipeline executed end-to-end on GPU:
1) train_transformer_teacher.py --teacher-init-path models/teacher_dapt --seeds 13,42,7
2) calibrate_teacher.py
3) cache_teacher_logits.py
4) train_transformer_student_distill.py
5) export_transformer_student_onnx.py
6) quantize_transformer_student.py
7) evaluate_transformer.py

Key holdout comparison vs baseline models/student_holdout_eval.json:
- Scam precision: 0.9512 -> 0.9024 (worse)
- Scam recall: 0.7800 -> 0.7400 (worse)
- Scam FPR: 0.0122 -> 0.0244 (worse; doubled)
- Scam F1: 0.8571 -> 0.8132 (worse)
- Topic F1: 0.8000 -> 0.8430 (better)
- Macro F1: 0.8504 -> 0.8514 (slightly better)
- Exact match: 0.8598 -> 0.8645 (slightly better)

Thresholds shifted materially (scam 0.61 -> 0.78, topic 0.60 -> 0.50), indicating non-trivial calibration behavior change.

Decision: reject DAPT as default teacher init for now because primary objective (scam recall at low FPR) regressed materially.

## 00:54

Starting non-DAPT student distillation sweep with fixed teacher logits (models/teacher_logits_train.npz + models/teacher_logits_valid.npz), fixed data splits, and fixed preprocessing.
Grid:
- distill_temp: 2.0, 3.0, 4.0
- alpha (hard-loss weight): 0.5, 0.7, 0.85
Selection criterion: maximize scam recall while keeping holdout scam FPR <= 0.02.

Completed non-DAPT distillation hyperparameter sweep (9 runs) using fixed teacher logits and fixed data splits.
Experiment root:
- models/student_experiments/2026-02-14-student-sweep-nondapt

Grid:
- distill_temp: 2.0, 3.0, 4.0
- alpha (hard loss weight): 0.5, 0.7, 0.85
- seed fixed at 42

For each run, executed:
1) train_transformer_student_distill.py
2) evaluate_transformer.py (threshold tuning on valid, holdout reporting)

Baseline reference (models/student_holdout_eval.json):
- scam precision=0.9512
- scam recall=0.7800
- scam fpr=0.0122
- scam f1=0.8571
- macro f1=0.8504

Outcome summary:
- No sweep config improved scam recall at or below 2% FPR vs baseline.
- Best under FPR<=0.02 was t4_a05 (temp=4, alpha=0.5):
  - scam precision=0.9231
  - scam recall=0.7200
  - scam fpr=0.0183
  - scam f1=0.8090
- Highest recall overall was t3_a05 (0.7400) but FPR rose to 0.0305.

Decision:
- Keep current baseline student as default; this sweep regressed the primary objective (scam recall/F1 at low FPR).
- Next likely high-ROI training-only step is hard-example reweighting / focal-style loss on existing labels, since pure temp/alpha distillation sweeps did not improve the scam operating point.

## 01:07

Starting long-duration baseline student training experiment:
- Same baseline setup (non-DAPT teacher logits, baseline architecture/hyperparams)
- Increased epochs from 10 to 60
- Will capture epoch-by-epoch valid_macro_f1 trend and compare final holdout metrics against baseline.

Completed long-duration baseline training experiment:
- models/student_experiments/2026-02-14-baseline-longtrain-epochs60
- Baseline student config, 60 epochs (seed 42), non-DAPT teacher logits
- Full eval run completed

Epoch trend (valid_macro_f1 highlights):
- epoch 1: 0.6429
- epoch 2: 0.8279
- epoch 3: 0.8357
- epoch 10: 0.8270
- epoch 20: 0.8268
- epoch 30: 0.8341
- epoch 40: 0.8310
- epoch 50: 0.8241
- best epoch: 51 with valid_macro_f1=0.8430
- epoch 60: 0.8252

Holdout comparison vs baseline models/student_holdout_eval.json:
- scam precision: 0.9512 -> 0.8947 (worse)
- scam recall: 0.7800 -> 0.6800 (worse)
- scam fpr: 0.0122 -> 0.0244 (worse; doubled)
- scam f1: 0.8571 -> 0.7727 (worse)
- topic f1: 0.8000 -> 0.8293 (better)
- macro f1: 0.8504 -> 0.8351 (worse)
- exact match: 0.8598 -> 0.8551 (worse)

Thresholds shifted upward materially (scam 0.61 -> 0.83; topic 0.60 -> 0.72).

Conclusion: training much longer did not help the primary operating point and appears to hurt scam generalization / calibration despite occasional late validation peaks.

## 10:40

Starting student capacity sweep to measure size/performance tradeoff.
Controls fixed: same teacher logits, same data splits, same training recipe (distill_temp=3.0, alpha=0.5, epochs=10, seed=42).
Variable changed: student architecture only.
For each architecture: train -> evaluate -> export ONNX -> quantize int8, then compare holdout scam metrics and artifact sizes.

Capacity sweep interruption: configuration h256_l6_a4_i1024 failed during student training.
Error: IndexError in train_transformer_student_distill.py when accessing teacher hidden states (index 4 out of bounds for size 4).
Cause: current distillation implementation expects student layer count <= teacher hidden layer cache depth (4).
Action: continue sweep with valid 4-layer architectures only to isolate size effects.

Completed student capacity sweep experiment.
Experiment root: models/student_experiments/2026-02-14-capacity-sweep

Valid architectures run (same training recipe, same teacher logits):
- h128_l4_a4_i512
- h192_l4_a4_i768
- h256_l4_a4_i1024
- h384_l4_a6_i1536

For each config executed: train -> evaluate -> ONNX export -> int8 quantize.

Holdout outcome summary vs baseline models/student_holdout_eval.json:
- h128_l4_a4_i512: int8 1.93MB, scam recall 0.74, scam FPR 0.0366, scam F1 0.7957
- h192_l4_a4_i768: int8 3.39MB, scam recall 0.68, scam FPR 0.0183, scam F1 0.7816
- h256_l4_a4_i1024: int8 5.23MB, scam recall 0.70, scam FPR 0.0305, scam F1 0.7778
- h384_l4_a6_i1536: int8 10.02MB, scam recall 0.72, scam FPR 0.0122, scam F1 0.8182

Baseline reference remains stronger for primary objective:
- baseline scam recall 0.78 at scam FPR 0.0122, scam F1 0.8571

Observed trend in this sweep: larger models increased size substantially and improved some metrics vs smaller sweep configs, but did not beat the established baseline operating point.

Failure encountered and diagnosed:
- h256_l6_a4_i1024 failed with IndexError due teacher hidden-state cache depth (4) being smaller than student layer count (6); current distillation implementation requires student layers <= cached teacher layer depth.

## 11:29

Backing up current split files before holdout expansion:
- data/split_backups/2026-02-14-holdout-expand/{train,valid,holdout}.{jsonl,txt}
Planned new split ratios from data/sample.jsonl:
- train=0.60, valid=0.10, calib=0.05, holdout=0.25
Goal: holdout around 1k rows with roughly 200-300 scam examples.

Completed holdout expansion.

Actions taken:
1) Rebuilt stratified splits from data/sample.jsonl using ratios:
   - train=0.60, valid=0.10, calib=0.05, holdout=0.25
   Command: uv run --project scripts python scripts/make_stratified_splits.py --input data/sample.jsonl --train-ratio 0.60 --valid-ratio 0.10 --calib-ratio 0.05 --holdout-ratio 0.25 --seed 42
2) Regenerated transformer prepared splits:
   - uv run --project scripts python scripts/prepare_transformer_data.py

Resulting prepared counts:
- train: 2566 (scam 587, clean 1216, topic_crypto 763)
- valid: 428 (scam 98, clean 203, topic_crypto 127)
- holdout: 1069 (scam 245, clean 506, topic_crypto 318)

Holdout target achieved: >1k rows with scam count in the 200-300 range.

Note: Existing model/eval artifacts were produced on the old holdout and are no longer directly comparable to new holdout metrics; retraining/re-evaluation is needed for apples-to-apples numbers.

## 12:50

Starting full retrain chain on expanded holdout splits.
Run root: models/student_experiments/2026-02-14-retrain-expanded-holdout
Steps: teacher(3 seeds 13,42,7) -> calibrate -> cache logits -> student distill -> ONNX export -> int8 quantize -> evaluate.
Using new prepared splits:
- data/transformer/train.prepared.jsonl (2566)
- data/transformer/valid.prepared.jsonl (428)
- data/transformer/holdout.prepared.jsonl (1069)

## 13:01

Added expanded-holdout retrain evaluation report:
- docs/reports/2026-02-14-tiny-transformer-expanded-holdout-retrain-report.md
Included: updated split sizes, full command chain, required teacher config (cardiffnlp/twitter-roberta-large-2022-154m, seeds 13/42/7), teacher and student holdout metrics, artifact inventory/sizes, and comparability caveat versus prior small-holdout baseline.
Preparing docs-only commit; model/data binaries remain untracked and will not be committed.

## 13:10

Ran apples-to-apples comparison on expanded holdout (1069 rows) by evaluating prior baseline student checkpoint (models/student) with current splits.
Command: uv run --project scripts python scripts/evaluate_transformer.py --student-dir models/student --train data/transformer/train.prepared.jsonl --valid data/transformer/valid.prepared.jsonl --holdout data/transformer/holdout.prepared.jsonl --thresholds-out models/student_experiments/2026-02-14-baseline-on-expanded-holdout/thresholds.transformer.json --out models/student_experiments/2026-02-14-baseline-on-expanded-holdout/student_holdout_eval.json
Result: prior baseline is substantially better than retrain-expanded-holdout run on the same holdout.
- Baseline-on-expanded: scam P/R/F1/FPR = 0.9488 / 0.8327 / 0.8870 / 0.0133; topic F1 = 0.8916; macro F1 = 0.9068; exact = 0.9149
- Retrain-expanded-holdout run: scam P/R/F1/FPR = 0.9383 / 0.6204 / 0.7469 / 0.0121; topic F1 = 0.7695; macro F1 = 0.7967; exact = 0.8176
Primary regression is recall/F1 collapse at similar scam FPR.

## 13:27

Root-cause analysis for apparent regression after holdout expansion:
- Prior baseline checkpoint (models/student) was trained on old split before reshuffle.
- New expanded holdout (1069) overlaps heavily with old splits:
  - new holdout ∩ old train = 847
  - new holdout ∩ old valid = 72
  - new holdout ∩ old holdout = 10
  - total overlap with old(any split) = 929 / 1069 (86.9%)
- Therefore baseline-vs-retrain comparison on expanded holdout is contaminated; baseline appears artificially strong because most of that holdout was previously seen by baseline during prior training/validation cycle.
- New split itself is clean (new holdout ∩ new train = 0; new holdout ∩ new valid = 0).
- Retrain also had reduced supervision due split change: old train 3421 -> new train 2566 (-25%), scam samples 783 -> 587 (-25%).
Interpretation: large measured regression versus prior baseline is largely an illusion from leakage; residual degradation likely from reduced train set and untuned student distillation settings for the new split regime.

## 15:00

Starting clean benchmark rebuild workflow on user request:
1) freeze current split as benchmark reference
2) add overlap/leakage guard
3) retrain fastText baseline from scratch on frozen split
4) retrain transformer teacher->student baseline from scratch on same frozen split
5) produce apples-to-apples comparison report.

Frozen benchmark split created at dataset/benchmarks/2026-02-14_expanded_holdout:
- train/valid/calib/holdout ID lists
- MANIFEST.json with row counts, label support, and SHA256 hashes
Added scripts/check_split_leakage.py and validated:
- current split pairwise overlap is zero across train/valid/calib/holdout
- previous-vs-current contamination check reproduces overlap in new holdout (847 old-train, 72 old-valid, 10 old-holdout)

Trained fastText baseline from scratch on frozen split data/train.txt.
- Model: models/benchmarks/2026-02-14_expanded_holdout/fasttext/scam_detector.bin
- Calibrated thresholds on data/calib.txt with target FPR 0.02 for topic_crypto/scam
- Evaluated on data/holdout.txt and wrote structured eval JSON:
  models/benchmarks/2026-02-14_expanded_holdout/fasttext/holdout_eval.json

## 15:12

Completed full transformer benchmark retrain on frozen split (teacher->calibration->logits->student->onnx->int8->eval).
Run root: models/benchmarks/2026-02-14_expanded_holdout/transformer
Teacher: cardiffnlp/twitter-roberta-large-2022-154m, seeds 13/42/7
Student holdout (torch): scam precision/recall/fpr/f1 = 0.9371 / 0.6082 / 0.0121 / 0.7376; macro_f1=0.7998; exact=0.8232
Student holdout (int8 onnx): scam precision/recall/fpr/f1 = 0.9375 / 0.6122 / 0.0121 / 0.7407; macro_f1=0.8013; exact=0.8241

Wrote frozen-split benchmark report:
- docs/reports/2026-02-14-frozen-split-fasttext-vs-transformer-benchmark.md
Includes: split freeze artifacts, leakage checks, full retrain commands, side-by-side holdout metrics (fastText vs transformer torch/int8), and benchmark conclusion.

Extended benchmark with fastText production-profile quantization:
- reduced fastText model to scam_detector.ftz (cutoff=1000,dsub=8), size ~123KB
- evaluated quantized fastText on holdout and added structured artifact holdout_eval_ftz.json
- updated report to compare transformer-int8 against fastText-ftz (production-relevant) in addition to noting unquantized .bin size.
