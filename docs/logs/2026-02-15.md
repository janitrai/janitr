---
title: Daily Log 2026-02-15
author: bob <unknown@example.com>
date: 2026-02-15
tz: Europe/Berlin
created: 2026-02-15T00:08:59+01:00
last_section: 2026-02-15T22:52:03+01:00
updated: 2026-02-15T22:52:03+01:00
---

## 00:08

Compared small transformer (`h128_l4_a4_i512`, int8 eval at `/tmp/small_h128_holdout_eval_int8.json`) against baseline transformer benchmark (`models/benchmarks/2026-02-14_expanded_holdout/transformer/student_holdout_eval_int8.json`).

Observed large gains on the shared holdout support (scam=245, clean=506, topic_crypto=318): scam recall +32.24 pts, scam F1 +21.74 pts, macro F1 +16.28 pts, exact match +14.22 pts, and lower scam FPR by 0.73 pts.

Flagged as likely needing provenance validation because gain magnitude is unusually large versus earlier transformer comparisons.

## 09:13

Backed up full local `models/` directory before commit/push to `/home/bob/model_backups/janitr_models_20260215_091229` (~32G snapshot).

Prepared commit including `data/split_backups/2026-02-14-holdout-expand/*` and todayâ€™s SimpleLog updates.

Per request, deleted oversized backup `/home/bob/model_backups/janitr_models_20260215_091229` and replaced it with a minimal backup containing only the deployable 2.0MB student int8 ONNX artifact.

New backup path: `/home/bob/model_backups/janitr_small_int8_20260215_091611/student.int8.onnx`.

## 09:30

Implemented dual-backend extension inference plumbing so Janitr can run `fasttext`, `transformer`, or `auto` backend selection without replacing the current flow.

Code changes:

- `extension/background.js`: added persisted backend config (`ic_model_backend`) via extension storage, plus runtime messages `ic-set-model-backend` and `ic-get-model-backend`; inference messages now forward selected engine to offscreen.
- `extension/offscreen.js`: added engine router with fastText path, transformer path, and `auto` fallback behavior (transformer first, then fastText if transformer fails).
- `extension/transformer/scam-detector-transformer.js` (new): ONNX runtime wrapper + BERT WordPiece tokenizer implementation in browser JS; outputs compatible Janitr result schema (`label`, `labels`, `scores`, `probability`, thresholds).
- `extension/manifest.json`: added `storage` permission and web-accessible paths for transformer/runtime assets.
- `extension/content-script.js`: now records returned inference engine/fallback metadata in debug logs.
- `.gitignore`: added ONNX/WASM binary ignore patterns.

Validation:

- Syntax checks passed with `node --check` for updated JS files.
- Playwright smoke test could not run in this environment because Chromium binary is not installed (`npx playwright install` required).

Staged local (git-ignored) transformer extension assets to make dual-backend path runnable without committing binaries:

- `extension/transformer/student.int8.onnx`
- `extension/transformer/thresholds.json`
- `extension/transformer/student_config.json`
- `extension/transformer/tokenizer/vocab.txt`
- `extension/vendor/onnxruntime-web/{ort.wasm.min.mjs,ort-wasm-simd-threaded.mjs,ort-wasm-simd-threaded.wasm}`

Updated `.gitignore` to keep these runtime/model artifacts local-only while allowing source code commits.

Attempted Node-based direct smoke run of transformer module; it failed on `fetch(file://...)` due Node undici file-scheme limitation. This does not block extension runtime (uses `chrome-extension://` URLs), but browser-side validation is still required.

## 09:36

Confirmed backend behavior: `auto` mode selects transformer first and falls back to fastText only on transformer failure.

Refactored extension naming to general classifier terminology:

- Renamed `extension/fasttext/scam-detector.js` -> `extension/fasttext/classifier.js`
- Renamed `extension/transformer/scam-detector-transformer.js` -> `extension/transformer/classifier-transformer.js`
- Renamed exports/usages to neutral API (`loadClassifierModel`, `loadClassifierThresholds`, `predictClassifier`, `resetClassifierModel`, `predictTransformerBatch`)
- Updated content-script UI/debug strings from "Scam model"/"Scam detector inference failed" to generic classifier phrasing and renamed confidence data attribute to `data-ic-confidence`
- Updated smoke tests and AGENTS.md references to new classifier names

Behavior remains unchanged aside from naming/output-key normalization (`isFlagged` instead of `isScam`).

Changed extension default classifier backend to transformer by setting `DEFAULT_ENGINE = ENGINE_TRANSFORMER` in `extension/background.js`.

Effect: when no backend is explicitly stored yet, inference requests now default to transformer path (with offscreen auto/explicit behavior unchanged).

## 09:45

Started full extension modern TypeScript migration completion pass: fixing TS compile errors in extension/src, adding global/module type declarations, and preserving runtime behavior with generated JS outputs.

## 09:52

Completed TypeScript migration of extension runtime sources under extension/src with typed implementations for background/content script/offscreen/fastText wrapper/transformer classifier.

Resolved path mapping issues by configuring extension/tsconfig.json rootDirs for source-vs-emitted tree parity.

Validation:

- npm run extension:typecheck (pass)
- npm run extension:build (pass; regenerated extension/\*.js runtime artifacts)

Ran Prettier on migrated TypeScript files and re-validated extension toolchain:

- npm run extension:typecheck (pass)
- npm run extension:build (pass)
- npx prettier --check on extension TS/build configs (pass)

Generated extension runtime JS artifacts are now produced from extension/src TypeScript sources.

Attempted runtime smoke test via Playwright (extension/tests/wasm-smoke.spec.ts). Failed because Chromium binary is not installed in this environment. Error indicates running 'npx playwright install' is required before browser-level extension tests.

Committed and pushed modern TypeScript extension rewrite on feat/tiny-transformer-impl. Commit: 8ca3bde (includes TS sources, build tooling, regenerated runtime JS, and migration notes in log).

## 10:01

Clarified metric mismatch: the previously cited ~90% transformer results are from baseline-on-expanded-holdout (1069 rows), while the 2MB capacity-sweep artifact metrics came from an older 214-row holdout eval. These are different runs/splits and not directly comparable.

## 10:20

Clarified that the 3.4MB transformer int8 artifact is not the ~90% run. Frozen-split int8 metrics are exact=0.8241, macro_f1=0.8013, scam_f1=0.7407; the ~90% figures came from the higher-scoring baseline-on-expanded-holdout eval context.

## 10:31

Summarized transformer capacity-sweep size/performance tradeoff on the shared eval set (n=214):

- h128_l4_a4_i512 (1.93MB int8): scam_f1 0.7957, scam_fpr 0.0366, macro_f1 0.7998, exact 0.8131
- h192_l4_a4_i768 (3.39MB int8): scam_f1 0.7816, scam_fpr 0.0183, macro_f1 0.8065, exact 0.8224
- h256_l4_a4_i1024 (5.23MB int8): scam_f1 0.7778, scam_fpr 0.0305, macro_f1 0.8233, exact 0.8411
- h384_l4_a6_i1536 (10.02MB int8): scam_f1 0.8182, scam_fpr 0.0122, macro_f1 0.8333, exact 0.8458

Also noted production benchmark int8 baseline (3.4MB, n=1069): macro_f1 0.8013, exact 0.8241, scam_f1 0.7407, scam_fpr 0.0121.

## 11:07

Acknowledged missed process adherence: AGENTS.md includes general-visualization skill breadcrumb for metric comparison outputs, and I failed to apply it on one response. Corrective action: default to general-visualization format for future comparison prints in this repo.

Updated AGENTS.md Skill Breadcrumbs to make general-visualization usage explicit as a NON-VIOLABLE rule with MUST wording for metric comparison outputs.

Printed combined visualization request: earlier transformer capacity sweep stats plus latest fastText metrics, with aligned plain-text comparison and best-value asterisk markers; included comparability caveat about split-size mismatch.

## 11:52

Recommendation for current transformer ship candidate: use benchmark int8 model (models/benchmarks/2026-02-14_expanded_holdout/transformer/student.int8.onnx, ~3.4MB) with matching tokenizer/config/threshold artifacts because it meets size target range and keeps scam FPR near 1.2% on the clean frozen benchmark holdout.

## 12:00

Proposed long-term model artifact directory structure for ongoing transformer/fastText experiments: separate immutable runs, promoted candidates, and production pointers; keep binaries out of git and track provenance manifests/metrics in each run.

## 14:48

Clarified industry practice for model artifact organization and Hugging Face publishing: keep immutable local run registry, publish only promoted checkpoints to Hub with model card/provenance, and pin deployment to exact Hub revision SHA.

Provided concrete Hugging Face upload workflow for current transformer ship candidate: auth, repo create, stage minimal deploy artifacts (int8 onnx + tokenizer + config + thresholds + eval), upload via huggingface_hub HfApi.upload_folder, and pin deployment to commit SHA/revision.

## 14:56

Per user request, rolled back all hover-nudge intervention footprints: removed hover patch/revert entries from docs/logs/2026-02-15.md and confirmed no code diffs remain in extension/src/content-script.ts or generated extension/content-script.js.

User created Hugging Face model repo for ongoing experiments: https://huggingface.co/janitr/experiments. Treat this as the continuous experiment upload target.

## 15:06

Researched Hugging Face access controls for safe delegation: documented recommendation to use fine-grained token scoped to janitr/experiments, upload via create_pr + parent_commit, and noted HF does not expose GitHub-style branch protection in current public repo settings docs (repo/org role controls and optional PR/discussion toggles are available).

## 15:15

Provided strict HF token setup checklist from screenshot options: fine-grained token, zero broad user/org scopes, repo-specific permissions only for janitr/experiments, and PR-only upload workflow with parent_commit + unique path to prevent clobber/overwrite.

Clarified branch-protection expectation on Hugging Face: write-scoped token can still push to main; Hub supports PR workflow but no GitHub-style mandatory branch protection controls exposed in standard repository settings docs. Recommended process controls: PR-only uploads, parent_commit guard, append-only run paths, short-lived token rotation.

## 15:21

Explained no-write-token handoff workflow: agent prepares staged artifacts and exact upload command/script locally; user executes final Hugging Face upload command with private token, preserving control over main branch writes.

Added `scripts/sync_to_experiments_repo.py` to support safe handoff syncing into a local experiments repo (default dest: ~/offline/janitr-experiments).

Behavior:

- Copies benchmark transformer deployment artifacts into `runs/<run_id>/`.
- Generates `MANIFEST.json` with file hashes/sizes/provenance.
- Refuses to clobber existing run dir unless `--force`.
- Validates destination is a git repo by default (override with `--allow-non-git`).
- Supports `--dry-run` and optional `--include-fp32`.

Validation:

- dry-run to temp destination (pass)
- full copy into temp git repo (pass)
- formatted script with `uvx ruff format`.

## 15:30

Clarified execution model confusion: assistant commands run in the same workspace machine as user (/home/bob/janitr), so sync script is local copy only; for remote-server to personal-laptop transfer user must use scp/rsync separately.

Added remote-to-local sync helper script scripts/sync_experiments_from_remote.sh for user laptop workflows. Script uses rsync over SSH, supports --remote, --remote-path, --dest, optional --run-id, --dry-run, and optional --delete, and excludes .git to avoid repo metadata clobbering.

Discussed run naming preference: user wants human-friendly codename style (e.g., adjective-animal) rather than purely technical IDs. Recommended dual naming with stable technical run_id plus optional codename alias.

## 15:45

Added automatic dated run-name support across training entrypoints.

- Added `scripts/run_naming.py` helper (`resolve_run_name`, `apply_run_name_template`) with enforced format: `YYYY-MM-DD-<petname>`.
- Wired `--run-name` into `scripts/train_fasttext.py`, `scripts/train_teacher_dapt.py`, `scripts/train_transformer_teacher.py`, and `scripts/train_transformer_student_distill.py`.
- Student distillation now resolves `{run_name}` in `--output-dir`, writes artifacts under the resolved path, and records `run_name` in `student_config.json`.
- Verified parser surfaces the new flag on all four scripts via `--help`.
- Verified helper behavior in `scripts/.venv`:
  - `resolve_run_name(None)` -> `2026-02-15-<petname>`
  - `resolve_run_name("flying-narwhal")` -> `2026-02-15-flying-narwhal`

## 15:59

Cloned Hugging Face experiments repo for handoff workflow:

- local path: /home/bob/offline/janitr-experiments
- origin: https://huggingface.co/janitr/experiments
- current branch: main
  This will be the staging location to copy model artifacts before user pulls to their local machine and performs HF upload.

Validated model handoff copy plan into Hugging Face experiments clone with dry-run:

- Command: python3 scripts/sync_to_experiments_repo.py --dest-root ~/offline/janitr-experiments --run-id 2026-02-15-flying-narwhal --dry-run
- Result: planned 9-file copy into runs/2026-02-15-flying-narwhal (model + tokenizer + thresholds + eval).

Committed and pushed dated run naming + experiments sync tooling updates.

- Commit: 19a2600 (feat/tiny-transformer-impl)
- Includes: auto `YYYY-MM-DD-<petname>` run-name helper, `--run-name` wiring across fastText/teacher/DAPT/student scripts, and sync helpers (`sync_to_experiments_repo.py`, `sync_experiments_from_remote.sh`).
- Pushed to origin branch: feat/tiny-transformer-impl

## 16:06

Added new repo-local skill: `.agent/skills/huggingface/SKILL.md`.

Content captures the canonical Hugging Face experiments workflow:

- remote staging via `scripts/sync_to_experiments_repo.py`
- default remote->local sync via `scripts/sync_experiments_from_remote.sh` without `--run-id`
- optional dry-run and optional single-run sync
- local commit/push steps from `~/offline/janitr-experiments`
  Also clarified that repo skills live under `.agent/skills` while `/home/bob/.codex/skills` currently only contains system skills.

## 16:17

Performed bulk artifact staging into /home/bob/offline/janitr-experiments/runs with policy enforcement after interrupted copy.

Policy applied per user request:

- Keep only runs traceable in git history.
- Remove any copied model artifact larger than 30MB.

Actions/results:

- Existing partially-copied runs from interrupted transfer were validated.
- Rewrote per-run RUN_INFO.json manifests (schema_version=2) with enforced constraints metadata and file hashes.
- Generated runs/INDEX.json summary with kept runs, removed runs, and removed large files.
- Kept runs: 14
- Removed runs as not traceable in git history: 14
- Removed oversized model files (>30MB): 2 files in h384 capacity-sweep run (`model/model.onnx`, `model/pytorch_model.bin`).
- Verified no files >30MB remain under /home/bob/offline/janitr-experiments/runs.

## 16:24

Corrected experiments staging after user clarification: only committed model artifacts should be included.

Actions:

- Discarded previous oversized staging under /home/bob/offline/janitr-experiments/runs.
- Rebuilt runs strictly from git history of committed trained model file `extension/fasttext/model.ftz`.
- Included companion thresholds from same commit (`extension/fasttext/thresholds.json`) when present.
- Extracted historical committed revisions with git dates into runs:
  - 2026-02-04-fasttext-976303b
  - 2026-02-07-fasttext-b0bb03d
  - 2026-02-11-fasttext-78c8255
  - 2026-02-11-fasttext-9d268fd
- Wrote per-run RUN_INFO.json with commit metadata and hashes.
- Wrote runs/INDEX.json with run list and policy metadata.
- Final staged runs size: ~648K total (~584,711 bytes payload + metadata), satisfying the "<1 MB committed models" expectation.

Added student model artifacts into ~/offline/janitr-experiments/runs using petname run IDs (`YYYY-MM-DD-<petname>`), while preserving existing committed fastText history runs.

Details:

- Added 18 student runs from local transformer/student experiment directories.
- Wrote per-run `RUN_INFO.json` mapping source run dir, copied files, hashes, total bytes, and skipped oversized model files.
- Enforced model file cap at 30MB (`max_model_file_mb=30`).
- One run had oversized artifacts skipped (`h384_l4_a6_i1536`): `student.onnx` and `pytorch_model.bin` were excluded; int8 model kept.
- Wrote aggregate index: `/home/bob/offline/janitr-experiments/runs/INDEX_STUDENT_PETNAMES.json`.
- Validation: no files under `runs/*/model/*` exceed 30MB.
- Current total size of runs folder after addition: ~388MB.

## 16:31

Corrected student artifact staging to include only relevant files for size variants after user feedback.

What changed:

- Removed prior 18 student petname runs that included excess artifacts.
- Re-copied only 4 capacity-sweep size variants with petnames:
  - 2026-02-15-lively-sparrow (h128)
  - 2026-02-15-glossy-lynx (h192)
  - 2026-02-15-apex-gazelle (h256)
  - 2026-02-15-gentle-koala (h384)
- Included only relevant files per run:
  - model/student.int8.onnx
  - model/student_config.json
  - model/tokenizer/{tokenizer.json, tokenizer_config.json, vocab.txt}
  - model/thresholds.json
  - eval/eval.json
- Kept previously staged committed fastText history runs untouched.
- New runs folder size dropped from ~388MB to ~23MB.
- Verified no model files >30MB remain.

## 16:42

Implemented single-index + validation system for experiments runs.

Code added:

- scripts/experiment_artifacts_model.py
  - Datamodel validators for `INDEX.json`, per-run `RUN_INFO.json`, and file entries.
- scripts/validate_experiment_runs.py
  - Enforces single-index policy (`INDEX.json` only), index/run manifest consistency, per-file size/hash checks, 30MB model cap, and empty-dir/untracked-file checks.
- scripts/rebuild_experiment_index.py
  - Rebuilds `INDEX.json` from run directories and removes extra `INDEX*.json` files by default.

Hook integration:

- Updated `.husky/pre-commit` to run experiment manifest validation when runs root exists:
  - `JANITR_EXPERIMENTS_RUNS_ROOT` override supported.
  - defaults to `$HOME/offline/janitr-experiments/runs`.

Applied to current artifacts:

- Ran `python3 scripts/rebuild_experiment_index.py --runs-root ~/offline/janitr-experiments/runs`
  - removed extra index file `INDEX_STUDENT_PETNAMES.json`
  - rebuilt single `INDEX.json` with 8 runs.
- Ran `python3 scripts/validate_experiment_runs.py --runs-root ~/offline/janitr-experiments/runs --require-runs-root`
  - validation passed with zero errors.

Updated `.agent/skills/huggingface/SKILL.md` to capture current artifact governance rules.

Added to skill:

- Single-index policy (`runs/INDEX.json` only).
- Required post-change steps: rebuild index (`scripts/rebuild_experiment_index.py`) and validate manifests (`scripts/validate_experiment_runs.py`).
- 30MB model artifact ceiling.
- Remote->local sync guidance now defaults to mirror mode with `--delete` to avoid stale/empty local run dirs.
- Canonical references to new scripts and updated workflow ordering.

## 16:51

Added programmatic dataset snapshot sync with auto petname naming and hash-based dedupe.

New script:

- `scripts/sync_datasets_to_experiments_repo.py`
  - Defaults: tweet=`data/sample.jsonl`, reply=`data/replies.jsonl`
  - Destination: `~/offline/janitr-experiments/datasets`
  - Snapshot ID auto-generated via `resolve_run_name()` (`YYYY-MM-DD-<petname>`)
  - Dedupe: computes combined hash of tweet+reply files and skips creating a new snapshot if unchanged
  - Writes per-snapshot manifest `SNAPSHOT.json` and updates `datasets/INDEX.json`

Execution:

- Ran sync once -> created `datasets/snapshots/2026-02-15-fierce-albatross`
- Ran sync again immediately -> dedupe hit, no duplicate snapshot created

Skill update:

- Updated `.agent/skills/huggingface/SKILL.md` to include dataset sync script, auto-petname naming for datasets, and no-duplicate behavior.

Renamed dataset snapshot naming from tweet/reply to X-native naming per user request.

Changes:

- `scripts/sync_datasets_to_experiments_repo.py`
  - CLI flags renamed to `--x-posts-source` and `--x-replies-source`.
  - Snapshot file names now `x-posts.jsonl` and `x-replies.jsonl`.
  - Manifest keys now `x-posts` and `x-replies`.
  - Console output updated accordingly.
- `.agent/skills/huggingface/SKILL.md` updated wording to x-posts/x-replies.

Migration applied to existing staged snapshot:

- Renamed files in `~/offline/janitr-experiments/datasets/snapshots/2026-02-15-fierce-albatross/`.
- Updated both `datasets/INDEX.json` and snapshot `SNAPSHOT.json` keys/destinations from tweet/reply to x-posts/x-replies.

Verification:

- Re-ran dataset sync script: dedupe still works and no duplicate snapshot created.

## 17:01

Hardened remote sync script to ignore git metadata directories during rsync.

Updated `scripts/sync_experiments_from_remote.sh`:

- Expanded rsync excludes to cover top-level and nested `.git` paths:
  - `.git`, `.git/`, `.git/**`, `**/.git`, `**/.git/`, `**/.git/**`
- Updated help note to clarify `.git` directories are excluded.

Validation:

- `bash -n scripts/sync_experiments_from_remote.sh` (pass)

Removed explicit `HUSKY=0` wording from repository docs per user request.

- Updated `docs/logs/2026-02-11.md` to replace the literal `HUSKY=0` mention with neutral wording (`temporary hook bypass`).
- Plan is to commit this change with hooks enabled (no HUSKY override).

## 17:08

Populated `~/offline/janitr-experiments/README.md` with practical usage details and explicit backlink to the main Janitr GitHub repo.

README now documents:

- purpose of experiments repo
- repository layout (`runs/`, `datasets/`)
- naming convention (`YYYY-MM-DD-<petname>`)
- integrity rules (single index, manifest checks, 30MB cap, dataset dedupe)
- canonical sync/validation commands from the main janitr repo scripts
- direct link to https://github.com/janitrai/janitr

Simplified experiments README per user request.

Updated `~/offline/janitr-experiments/README.md` to remove specific workflow/details and keep only:

- rolling-repo purpose statement for ongoing model/dataset experiments
- backlink to main Janitr GitHub repository
- brief note that contents change frequently

## 17:14

Updated `~/offline/janitr-experiments/README.md` to a single-line minimal statement per user request: "This is a repo for large files, model runs, and dataset checkpoints."

Restored `~/offline/janitr-experiments/README.md` to a concise informative version describing this as a rolling store for large artifacts (model runs/files and dataset checkpoints), with link back to main Janitr GitHub repo.

Clarified sync behavior: README syncs only on full-repo sync mode (no `--run-id`). If syncing with `--run-id`, only `runs/<id>/` is copied, so top-level files like README are not updated.

## 17:20

Attempted commit+push in local HF experiments clone (`~/offline/janitr-experiments`) for README + runs + datasets snapshot payload.

- Commit created locally on experiments repo: `38d3416` (branch `main`).
- Push failed due missing Hugging Face credentials in this environment:
  `fatal: could not read Username for 'https://huggingface.co': No such device or address`

User needs to authenticate (`hf auth login` or git credential/token setup) before pushing this local experiments commit.

Pulled and verified `~/offline/janitr-experiments` after user push.

- `git pull --ff-only` failed due divergence (local commit `38d3416` vs remote `c5cd12f`).
- Attempted rebase onto `origin/main`; encountered add/add conflicts on binary artifact files.
- Resolved by skipping duplicate local commit (`git rebase --skip`) and completing rebase.

Verification after sync:

- Local HEAD matches origin/main: `c5cd12f`.
- Working tree clean.
- README reflects rolling-artifacts text with GitHub backlink.
- `runs/INDEX.json` present with `run_count=8`.
- `datasets/INDEX.json` present with `snapshot_count=1` (`2026-02-15-fierce-albatross`).

## 17:37

Implemented advanced model management in the extension so transformer runs can be selected from UI and downloaded/activated from Hugging Face.

Changes:

- Added extension options UI (`extension/options.html`, `extension/options.css`, `extension/src/options.ts`) with:
  - backend selector (transformer/fasttext/auto)
  - remote run listing from HF repo index
  - run inspect (RUN_INFO + eval summary)
  - download+activate selected run
  - switch back to bundled transformer
  - cached-run listing and remove action
- Added transformer run registry/cache module (`extension/src/transformer/model-repo.ts`) with:
  - `runs/INDEX.json` and `RUN_INFO.json` fetch
  - required artifact validation for transformer runtime files
  - SHA-256 and size verification on download
  - IndexedDB cache for run files and metadata
  - active transformer source state in extension storage
  - fallback-safe active source resolver for offscreen runtime
- Extended background message API in `extension/src/background.ts`:
  - `ic-get-model-state`
  - `ic-list-remote-model-runs`
  - `ic-get-remote-model-run`
  - `ic-download-and-activate-transformer-run`
  - `ic-use-builtin-transformer`
  - `ic-list-cached-transformer-runs`
  - `ic-remove-cached-transformer-run`
- Updated offscreen runtime (`extension/src/offscreen.ts`) to load transformer assets from selected source and fall back to bundled transformer if cached source cannot load.
- Extended transformer loader (`extension/src/transformer/classifier-transformer.ts`) to support in-memory model/config/vocab/threshold payloads and cache keys.
- Updated manifest and build:
  - Added options page, HF host permissions, and connect-src entries
  - Added `unlimitedStorage` permission
  - Added new TS entrypoints in `extension/scripts/build.mjs`
  - Updated offscreen page title to general classifier phrasing

Validation:

- `npm run extension:build` passed
- `npm run extension:typecheck` passed

Post-implementation cleanup: ran Prettier on updated extension files, rebuilt generated JS artifacts, and re-ran extension build/typecheck. Current status: build + typecheck clean after formatting.

Updated main README to document Hugging Face experiment workflow and extension model selection approach.

README changes:

- Clarified local inference with optional advanced-mode model downloads.
- Updated "How It Works" to include transformer backend (default) and no network calls during inference.
- Added "Hugging Face Experiment Runs" section with `janitr/experiments` repo link, purpose, and run index/manifest structure.
- Documented advanced options flow: list runs, inspect metadata/eval, download+activate run, SHA-256/size verification, local IndexedDB cache, and fallback to bundled transformer.

## 17:51

Decoupled janitr pre-commit from external experiments repo path. .husky/pre-commit no longer defaults to /home/bob/offline/janitr-experiments/runs and now runs experiment manifest validation only when JANITR_EXPERIMENTS_RUNS_ROOT is explicitly set.

Removed experiment manifest validation from .husky/pre-commit entirely, per request to keep janitr hooks fully independent from experiments repo concerns.

Added extension action popup for model selection on icon click.

Changes:

- Set `manifest.action.default_popup` to `popup.html`.
- Added popup UI files: `extension/popup.html`, `extension/popup.css`, `extension/src/popup.ts`.
- Popup supports: backend selection, active source display, HF run listing/refresh, run activation, switch to bundled transformer, and open full options page.
- Added popup TS entrypoint to `extension/scripts/build.mjs`.
- Fixed TS global symbol collision by marking `options.ts` and `popup.ts` as modules (`export {}`).

Validation:

- `npm run extension:build` passed
- `npm run extension:typecheck` passed

## 18:00

Debugged extension runtime fetch failures after reload. Root cause: critical bundled transformer and onnxruntime-web assets were git-ignored, so fresh checkouts/reloads lacked required files (`student.int8.onnx`, tokenizer/config/thresholds, and ONNX Runtime web `.mjs/.wasm`).

Applied fix in `.gitignore` by adding explicit unignore rules for required extension assets so they are tracked and shipped with the extension package.

Aligned with request to avoid committing binaries:

- Reverted temporary `.gitignore` unignore change for transformer/onnxruntime binary assets.

Improved runnability without bundled transformer binaries:

- Updated offscreen inference routing so explicit `transformer` mode now gracefully falls back to fastText if transformer model/runtime load fails.
- This prevents repeated runtime failures (`Failed to fetch` / ORT import errors) from breaking classification on clean checkouts.

Validation:

- `npm run extension:build` passed
- `npm run extension:typecheck` passed

## 18:28

Implemented missing ORT runtime integration for transformer activation without committing binaries.

Changes:

- `downloadAndActivateTransformerRun()` now verifies a reachable ONNX Runtime wasm base URL in Hugging Face repo before activation.
- Added runtime path discovery (candidate dirs):
  - `runtime/onnxruntime-web/`
  - `vendor/onnxruntime-web/`
- Persist runtime wasm base URL in cached run metadata (`runtime_wasm_base_url`).
- `resolveTransformerAssetsForSource()` now requires runtime wasm base URL and passes it into transformer model load options.
- `classifier-transformer.ts` now accepts `ortWasmPathPrefix` and configures `ort.env.wasm.wasmPaths` from that value (fallback to local vendor path).
- Kept binary policy: wasm remains ignored; only non-binary ORT `.mjs` files were unignored for tracking.

Validation:

- `npm run extension:build` passed
- `npm run extension:typecheck` passed

Copied ONNX Runtime web assets into local HF experiments clone at ~/offline/janitr-experiments/runtime/onnxruntime-web (ort-wasm-simd-threaded.wasm, ort-wasm-simd-threaded.mjs, ort.wasm.min.mjs) and generated RUNTIME_INFO.json with hashes/sizes so user can sync/push.

## 18:34

Pulled ~/offline/janitr-experiments after user pushed runtime files. Pull initially blocked by untracked local runtime files; moved local runtime directory aside, fast-forwarded to origin/main (7a66c66), and retained backup at runtime/onnxruntime-web.local-prepull-20260215183437 because files differed from pulled working tree representation.

Could not delete the local runtime backup directory directly due shell policy restrictions on destructive commands in this environment. Moved backup out of HF clone to /home/bob/onnxruntime-web.local-prepull-20260215183437 instead, leaving ~/offline/janitr-experiments clean.

Prepared final fix for transformer loader import failure seen in extension console.

What this commit will ship:

- Track required onnxruntime-web loader modules in repo (`ort.wasm.min.mjs`, `ort-wasm-simd-threaded.mjs`) while still keeping `.wasm` binaries out of git.
- Keep transformer runtime wasm location externalized to HF runtime path via `ortWasmPathPrefix` wiring.
- Ensure `download + activate` stores runtime wasm base URL and `resolveTransformerAssetsForSource()` passes it into transformer loader.
- Added `extension/vendor/` to `.prettierignore` to avoid formatting third-party vendor modules.

## 19:09

Resolved repository format-check failure. Applied Prettier to the reported 10 files, then applied Ruff formatting to 13 Python scripts reported by uvx ruff format --check. Verified full command now passes: npm run format:check.

Switched formatting/tooling to uv everywhere: package scripts and lint-staged now use uvx ruff, and CI now installs uv via astral-sh/setup-uv and runs Python checks through uv run to avoid missing uvx/python path issues in GitHub Actions.

## 19:14

Pinned CI uv toolchain version to 0.7.3 in both GitHub Actions jobs via astral-sh/setup-uv to keep formatter/lint execution deterministic across runners.

## 19:35

Pulled latest origin/main (fast-forwarded local main to 5082240) and switched back to feat/tiny-transformer-impl to keep branch context unchanged.

Updated root README to reflect current state: transformer is default backend with fastText fallback, dataset size updated to ~4.2k+, model performance section replaced with frozen-split fastText vs transformer benchmark snapshot, advanced mode/backend switch behavior clarified, and training commands standardized to uv-based fastText + transformer pipelines.

Refined README consistency: extension development section now points to TypeScript source/runtime directories, and Local-First section now explicitly distinguishes local inference from optional user-triggered model downloads in advanced mode.

Removed the Hugging Face advanced-mode clause from the opening README sentence; intro now states only the core local on-device inference promise.

## 20:56

Opened a fresh PR branch from updated main to avoid previously merged/overlapping history. Created and pushed docs/readme-transformer-refresh-2026-02-15 with README transformer-first refresh changes and validation via npm run format:check.

## 21:03

Opened new README refresh PR using GitHub App credentials from ~/.config/github-app helper (in-memory token only). PR URL: https://github.com/janitrai/janitr/pull/23.

## 21:19

Changed extension inference to fail hard on transformer errors (no automatic fastText fallback) and clear transformer runtime cache state on failure so stale rejected model promises do not cause persistent fallback loops. Also removed silent HF-run->builtin source fallback in offscreen and normalized legacy 'auto' backend values to transformer; removed 'auto' from popup/options backend selectors to avoid fallback expectations.

Opened PR #24 from fix/transformer-no-fasttext-fallback to main after branch-protection rejection on direct main push. This PR hard-disables transformer->fastText fallback and surfaces transformer errors to users for direct diagnosis.

## 21:29

Reduced extension UI paragraph spacing in options/popup styles and increased content-script debug text preview truncation from 200 to 500 characters for inference logs.

## 21:38

Fixed transformer runtime fetch path: model-repo no longer configures ort wasm paths to Hugging Face URLs for builtin/HF runs; runtime now uses bundled extension ORT assets to avoid 'Failed to fetch' during inference. Added transformer smoke test harness (test page + TS runner + Playwright spec) that downloads/activates a transformer run via extension runtime messages and asserts an obvious scam sample is classified as scam with engine=transformer. Updated extension build entrypoints to emit transformer-smoke test script.

## 21:54

Investigated persistent transformer 'Failed to fetch' after merge. Root cause identified: extension package does not include ORT wasm binary, so forcing local wasm path breaks ONNX init. Updated transformer runtime resolution to use Hugging Face-hosted ORT wasm base URL (runtime/onnxruntime-web first, vendor/onnxruntime-web fallback), persisted per downloaded run, and reused during cached run inference. Added richer transformer fetch/session error messages with label+URL and wasmPaths context to expose the exact failing asset instead of generic TypeError. Rebuilt generated JS, format/type checks pass; transformer smoke Playwright spec remains skipped in this headless/no-DISPLAY environment.

Opened PR #25 (fix/transformer-wasm-runtime-url) without merging. It restores HF-hosted ORT wasm path resolution (required because wasm binary is not shipped in the extension package), stores runtime_wasm_base_url in cached run metadata, and improves error context for failed fetch/ONNX session init. Corrected PR body after initial shell interpolation stripped inline literals.

## 22:13

Implemented runtime/model artifact split for transformer delivery.

Changes made:

- Pinned `onnxruntime-web` in root `package.json` (`1.24.1`) for deterministic extension runtime assets.
- Updated `extension/scripts/build.mjs` to stage required ORT runtime files from `node_modules/onnxruntime-web/dist` into `extension/vendor/onnxruntime-web/` during `npm run extension:build`:
  - `ort.wasm.min.mjs`
  - `ort-wasm-simd-threaded.mjs`
  - `ort-wasm-simd-threaded.wasm`
- Switched `.gitignore` so ORT runtime assets under `extension/vendor/onnxruntime-web/` are treated as build outputs (kept `.gitkeep` only).
- Removed committed ORT runtime module files from git tracking; build now owns these artifacts.
- Refactored `extension/src/transformer/model-repo.ts` (+ regenerated JS) to remove Hugging Face runtime-wasm probing and storage coupling:
  - removed runtime wasm URL discovery/probing against HF repos
  - removed `runtime_wasm_base_url` write/use path
  - HF run activation now downloads only model artifacts required by `RUN_INFO.json` (ONNX/config/vocab/thresholds)
  - ORT wasm path now resolves from extension-local runtime assets only.
- Updated `README.md`:
  - install flow now explicitly includes `npm install` + `npm run extension:build`
  - documented app/runtime vs HF artifact scope boundary
  - documented that extension build stages ORT runtime from pinned dependency.

Validation run:

- `npm install --no-audit` (pass)
- `npm run extension:build` (pass; ORT runtime files staged into extension vendor dir)
- `npm run format` (pass)
- `npm run format:check` (pass)
- `npm run extension:typecheck` (pass)
- `npx playwright test extension/tests/transformer-smoke.spec.ts --reporter=line` (pass with 1 skipped due no DISPLAY in this environment)

Resulting architecture now matches requested best-practice split:

- janitr repo owns runtime/app dependencies and build-time ORT asset staging
- Hugging Face experiments repo remains model/data artifacts only.

## 22:19

Pushed runtime/build split refactor to `fix/transformer-wasm-runtime-url` and confirmed PR linkage.

- Commit: `7ba4440` (`refactor(extension): stage ORT runtime in build and keep HF model-only`)
- Push: origin/fix/transformer-wasm-runtime-url updated
- Existing open PR detected and verified to include HEAD:
  - PR #25: https://github.com/janitrai/janitr/pull/25
  - PR head SHA matches local HEAD (`7ba4440`)
- Note: GitHub CLI is not authenticated in this environment, but branch push succeeded and PR #25 auto-updated from branch head.

Performed real browser verification for transformer path after ORT runtime/build split.

- Installed `xvfb` in environment to enable headed extension tests without physical display.
- Ran: `xvfb-run -a npx playwright test extension/tests/transformer-smoke.spec.ts --reporter=line`
- Result: `1 passed` (3.5s)
- This confirms extension service worker/offscreen inference path works in Chromium with transformer model activation and scam sample detection.

Cleaned temporary local test artifacts created during ad-hoc verification (`.tmp`, `.tmp-playwright-ext-profile`).

## 22:30

Handled local install failure report on macOS (`npm install` error: `Cannot read properties of null (reading 'matches')` with Node 24 / npm 11.3.0) by making pnpm first-class for this repo.

Implemented pnpm support:

- Added package manager pin in `package.json`: `"packageManager": "pnpm@10.23.0"`.
- Generated and committed `pnpm-lock.yaml`.
- Updated install/build docs in `README.md` to use:
  - `corepack enable`
  - `pnpm install`
  - `pnpm extension:build`
- Updated CI workflow (`.github/workflows/ci.yml`) to use pnpm:
  - setup step: `pnpm/action-setup@v4` with `10.23.0`
  - install: `pnpm install --frozen-lockfile`
  - lint: `pnpm run format:check`
  - simpledoc: `pnpm dlx @simpledoc/simpledoc check`

Validation run (local):

- `pnpm install` (pass)
- `pnpm extension:build` (pass)
- `pnpm run format` (pass)
- `pnpm run format:check` (pass)
- `pnpm run extension:typecheck` (pass)

Note observed during install:

- pnpm printed "Ignored build scripts" warning for some deps; despite that, `pnpm extension:build` succeeded and esbuild resolved correctly in this environment.

Follow-up hardening for pnpm v10 install behavior.

- Added `pnpm.onlyBuiltDependencies` allowlist to `package.json` for:
  - `esbuild`
  - `protobufjs`
- Ran `pnpm rebuild` once to execute previously deferred postinstall scripts.
- Re-ran `pnpm install` and confirmed warning about ignored build scripts is gone.

This avoids interactive `pnpm approve-builds` prompts for required build-time deps and keeps setup non-blocking.

## 22:44

Fixed transformer runtime failure when bundled `extension/transformer/*` artifacts are absent.

User-reported error:

- `Failed to fetch transformer student config from chrome-extension://.../transformer/student_config.json`
- same for `transformer/thresholds.json`

Root cause:

- Active source can be `builtin` while bundled transformer model artifacts are not present in the local extension package.
- In that state, offscreen inference attempts to fetch local `chrome-extension://.../transformer/...` files and fails hard.

Code changes:

- `extension/src/transformer/model-repo.ts`
  - Added bundled-asset availability probing for:
    - `transformer/student.int8.onnx`
    - `transformer/student_config.json`
    - `transformer/tokenizer/vocab.txt`
    - `transformer/thresholds.json`
  - If bundled assets are missing:
    - auto-select newest cached HF run (if available)
    - persist that source via `setActiveTransformerSource()`
    - continue inference with cached run assets
  - If bundled assets missing and no cached run exists:
    - throw explicit actionable error instructing user to download+activate a run from Options.
- `extension/src/tests/transformer-smoke.ts`
  - After downloading/activating a run, explicitly switches source to built-in (`ic-use-builtin-transformer`) before inference.
  - This validates the new auto-switch behavior from missing built-in -> cached HF run.
- `README.md`
  - Clarified runtime/model behavior and added note about auto-selecting newest cached HF run when bundled transformer artifacts are absent.

Validation:

- `pnpm extension:build` (pass)
- `pnpm run extension:typecheck` (pass)
- `xvfb-run -a npx playwright test extension/tests/transformer-smoke.spec.ts --reporter=line` (pass, 1 passed)

## 22:52

Updated extension build pipeline to auto-format generated JS outputs.

Problem addressed:

- Users were seeing local git conflicts on generated extension files after running build (`extension/tests/transformer-smoke.js`, `extension/transformer/model-repo.js`, etc.) because generated output style could drift until a separate formatter run.

Change:

- `extension/scripts/build.mjs` now formats generated output files with Prettier immediately after esbuild runs.
- Added direct Prettier API usage (`resolveConfig` + `format`) over the exact generated JS output list.
- Updated missing-runtime install hint to pnpm-native wording.

Result:

- `pnpm extension:build` now leaves generated files in the same formatting state expected by `pnpm run format:check`, reducing incidental pull/merge conflicts.

Validation:

- `pnpm extension:build` (pass)
- `pnpm run format:check` (pass)
